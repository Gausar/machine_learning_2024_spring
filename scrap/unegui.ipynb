{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cars:\n",
    "    def __init__(self, mark_, price_, produced_year_, imported_year_, distance_, motor_volume_, color_, type_, hurd, hodolguur_):\n",
    "        self.mark = mark_\n",
    "        self.price = price_\n",
    "        self.produced_year = produced_year_\n",
    "        self.imported_year = imported_year_\n",
    "        self.distance = distance_\n",
    "        self.motor_volume = motor_volume_\n",
    "        self.color = color_\n",
    "        self.type = type_\n",
    "        self.hurd = hurd\n",
    "        self.hodolguur = hodolguur_\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseurl = \"https://www.unegui.mn/avto-mashin/-avtomashin-zarna/?page=\"\n",
    "car_list = []\n",
    "for i in range(1, 200):\n",
    "    url = baseurl + str(i)\n",
    "\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(response.status_code)\n",
    "        print('error', url)\n",
    "        continue\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    li_list = soup.find_all(\"div\", {\"class\": \"swiper-wrapper\"})\n",
    "    for li in li_list:\n",
    "        a = li.find('a')\n",
    "        car_url = \"https://www.unegui.mn\" + a['href']\n",
    "        #print(car_url)\n",
    "        car_list.append(car_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findFeature(li_list, header):\n",
    "    ref = None\n",
    "    for li in li_list:\n",
    "        text = li.text.strip()\n",
    "        if text.startswith(header):\n",
    "            ref = text[len(header):].strip()\n",
    "            break\n",
    "    return ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11940\n",
      "11940\n"
     ]
    }
   ],
   "source": [
    "print(len(car_list))\n",
    "car_set = set(car_list)\n",
    "print(len(car_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = 0\n",
    "cars_data = []\n",
    "for url in car_set:\n",
    "    it += 1\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(response.status_code)\n",
    "        print('error', url)\n",
    "        continue\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    mark = soup.find(\"h1\", {\"class\": \"title-announcement\"}).text.strip()\n",
    "    mark = str(mark.split(',')[0])\n",
    "    price = soup.find(\"div\", {\"class\": \"announcement-price__cost\"}).text.strip()\n",
    "    price = float(price.split('сая')[0])\n",
    "    li_class = soup.find_all(\"li\")\n",
    "    produced_year = findFeature(li_class, \"Үйлдвэрлэсэн он:\")\n",
    "    imported_year = findFeature(li_class, \"Орж ирсэн он:\")\n",
    "    distance = findFeature(li_class, \"Явсан:\")\n",
    "    motor_volume = findFeature(li_class, \"Мотор багтаамж:\")\n",
    "    color = findFeature(li_class, \"Дотор өнгө:\")\n",
    "    type = findFeature(li_class, \"Төрөл:\")\n",
    "    hurd = findFeature(li_class, \"Хүрд:\")\n",
    "    hodolguur = findFeature(li_class, \"Хөдөлгүүр:\")\n",
    "    \n",
    "    \n",
    "    #print(title, price, p_year, i_year, distance, motor, color)\n",
    "    car = Cars(mark, price, produced_year, imported_year, distance, motor_volume, color, type, hurd, hodolguur)\n",
    "    cars_data.append(car.__dict__)\n",
    "    #print(car)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(cars_data)\n",
    "df.to_csv('car_info.tsv', sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unegui.mn - ээс 11940-н авто машинтай холбоотой зар байсан. Гэхдээ scrap хийхэд нэлээн удаж хийгдсэн, тэрнээс болоод зөвхөн 5076 машины зар авч чадлаа.\n",
    "Файл нь доорх холбоост байгаа. \n",
    "\n",
    "https://drive.google.com/file/d/1CKWn_97UA7gUfx1THx50cbMwWRAbSAvS/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Nomin.mn сайтаас электроник барааны зарын мэдээллийг татсан. Гэвч энэ 6-н баганатай."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Product:\n",
    "    def __init__(self, name_, brand_, sale_day, sale_price_, sale_percent_, additionally_):\n",
    "        self.name = name_\n",
    "        self.brand = brand_\n",
    "        self.sale_day = sale_day\n",
    "        self.sale_price = sale_price_\n",
    "        self.sale_percent = sale_percent_\n",
    "        self.additionally = additionally_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseurl = 'https://enomin.mn/t/6011?page='\n",
    "product_list = []\n",
    "for i in range(1, 63):\n",
    "    url = baseurl + str(i)\n",
    "    #print(url)\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(response.status_code)\n",
    "        print('error ', url)\n",
    "        continue\n",
    "    #print(response.text)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    li_list = soup.find_all(\"div\", {\"class\": \"MuiBox-root css-1efcy4n\"})\n",
    "    for li in li_list:\n",
    "        a = li.find('a')\n",
    "        product_url = \"https://enomin.mn\" + a['href']\n",
    "        #print(product_url)\n",
    "        product_list.append(product_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1860"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(product_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first2(s):\n",
    "    return s[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_data = []\n",
    "it  = 0\n",
    "for url in product_list:\n",
    "    it += 1\n",
    "    #print(url)\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        #print(response.status_code)\n",
    "        print('error ', url)\n",
    "        continue\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    name = soup.find(\"h1\", {\"class\": \"MuiBox-root css-1ozh2ah\"}).text.strip()\n",
    "    brand = soup.find(\"h6\", {\"class\": \"MuiBox-root css-jyp6ua\"}).text.strip()\n",
    "    sale_day = soup.find(\"div\", {\"_2EBbg\"}).text.strip()\n",
    "    sale_day = first2(sale_day)\n",
    "    sale_price = soup.find(\"h2\", {\"class\": \"MuiBox-root css-x5yzb2\"}).text.strip()\n",
    "    sale_percent = soup.find(\"span\", {\"class\": \"MuiChip-label MuiChip-labelSmall css-19imqg1\"}).text.strip()\n",
    "    additionally = soup.find(\"div\", {\"class\": \"MuiBox-root css-10ibhvy\"}).text.strip()\n",
    "\n",
    "    products = Product(name, brand, sale_day, sale_price, sale_percent, additionally)\n",
    "    product_data.append(products.__dict__)\n",
    "    #print(name, brand, sale_day, sale_price, description, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(product_data)\n",
    "df.to_csv('electronic.tsv', sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мөн 1860 зараас 823-ыг нь татаж чадсан. Энэ удаад татагдаж байсан ч дундаас нь алдаа заагаад хагас нь татагдаагүй. Тэр ямар учиртай юм бол багшаа.\n",
    "\n",
    "https://drive.google.com/file/d/1E13u17TFsmz8LlrmPIPq0vGjAjdeeT_B/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мөн багшаа зарим нэгэн сайтууд руу ороход тэдний вэб inspect хийгдэхгүй байсан. \n",
    "1. Тэдний HTML кодыг харах ямар нэгэн арга байгаа юу?\n",
    "2. Тэр HTML код ямар учраас inspect хийгдэхгүй байгаа вэ? Ямар арга ашиглаж тийм болгодог бол?\n",
    "Жишээ нь : emonos.mn сайт \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
